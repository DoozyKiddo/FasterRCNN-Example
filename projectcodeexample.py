# -*- coding: utf-8 -*-
"""Project Code Step 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M5JK7RFXkC6CC50CBYab2lYDQkU2tyu2
"""

#Setting up google drive (requires authentication code)

from google.colab import drive
root = '/content/drive/'
drive.mount(root)

#Import necessary packages

import matplotlib
# use this loop if matplotlib has an error specifying the backend TkAgg (optional)
i = 0
while i < 10:
    i += 1
    try:
        matplotlib.use('TkAgg')
        break
    except:
        print(i)
import matplotlib.pylab as plt
import cv2
import torch
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision.models.detection.rpn import AnchorGenerator
from datetime import datetime
import os
from os.path import join
import numpy as np
from PIL import Image
import torchvision
import torchvision.models as models
import torchvision.utils as utils
import sys
import json
import collections
from pprint import pprint
from torchvision import transforms as tr
import tensorflow as tf
from tqdm import tqdm
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
# code for installing pytorch3d
if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):
    !pip install pytorch3d
else:
    need_pytorch3d=False
    try:
        import pytorch3d
    except ModuleNotFoundError:
        need_pytorch3d=True
    if need_pytorch3d:
        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz
        !tar xzf 1.10.0.tar.gz
        os.environ["CUB_HOME"] = os.getcwd() + "/cub-1.10.0"
        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'

# Util functions for loading Meshes structures for object geometry
from pytorch3d.io import load_obj
from pytorch3d.io import load_objs_as_meshes

# Data structures and functions for rendering
from pytorch3d.structures import Meshes
from pytorch3d.renderer import (
	#data structure for loaded texture images
    Textures,
    look_at_view_transform,
    OpenGLPerspectiveCameras, 
    PointLights, 
    DirectionalLights, 
    Materials, 
    RasterizationSettings, 
    MeshRenderer, 
    MeshRasterizer,  
    SoftPhongShader,
    SoftSilhouetteShader,
    BlendParams
)
"""
#Use this if there is ever an error
!pip uninstall scikit-image
!pip uninstall imgaug
!pip install imgaug
!pip install -U scikit-image
"""
import skimage
print(skimage.__version__)

########################## LOAD GPU RESOURCES ##########################
# use_cuda - boolean flag to use CUDA if desired and available
use_cuda=False
print("CUDA Available: ",torch.cuda.is_available())

class_arr = []
class_quantized_arr = []

## Get ID of default device
print("ID of default device: ",torch.cuda.current_device() if torch.cuda.is_available() else "cpu")

#Set device
if (use_cuda and torch.cuda.is_available()):
	os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
	os.environ["CUDA_VISIBLE_DEVICES"]="0"
	device = torch.device("cuda")
#Uses cpu if cuda is not available or desired
else:
	device = torch.device("cpu")

class PrepareInstance(object):
    CLASSES = (
        "__background__ ",
        "aeroplane",
        "bicycle",
        "bird",
        "boat",
        "bottle",
        "bus",
        "car",
        "cat",
        "chair",
        "cow",
        "diningtable",
        "dog",
        "horse",
        "motorbike",
        "person",
        "pottedplant",
        "sheep",
        "sofa",
        "train",
        "tvmonitor",
    )
    def __call__(self, image, target):
        anno = target['annotation']
        h, w = anno['size']['height'], anno['size']['width']
        boxes = []
        classes = []
        area = []
        iscrowd = []
        objects = anno['object']
        if not isinstance(objects, list):
            objects = [objects]
        for obj in objects:
            bbox = obj['bndbox']
            bbox = [int(bbox[n]) - 1 for n in ['xmin', 'ymin', 'xmax', 'ymax']]
            boxes.append(bbox)
            classes.append(self.CLASSES.index(obj['name']))
            iscrowd.append(int(obj['difficult']))
            area.append((bbox[2] - bbox[0]) * (bbox[3] - bbox[1]))

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        classes = torch.as_tensor(classes)
        area = torch.as_tensor(area)
        iscrowd = torch.as_tensor(iscrowd)

        image_id = anno['filename'][5:-4]
        image_id = torch.as_tensor([int(image_id)])

        target = {}
        target["boxes"] = boxes
        target["labels"] = classes
        target["image_id"] = image_id

        # for conversion to coco api
        target["area"] = area
        target["iscrowd"] = iscrowd

        return image, target

########################## FUNCTIONS ##########################

## Function loading a 3D object and texture using PyTorch3D
#
#f_obj: path to a .obj file containing object geometry
#f_tex: path to an image file containing texture for f_obj
#returns: Meshes object representing 3D object geometry
def load_obj_and_texture_img_as_mesh(f_obj, f_tex, device=None, texture_requires_grad=False):
  #Load object data from .obj file
  verts, faces, aux = load_obj(f_obj, load_textures=False)
  verts = verts.to(device)
  
  if aux is None or aux.verts_uvs is None:
    raise ValueError(f"{f_obj} and {f_tex} loaded incorrectly")
  
  #Load Textures object using texture image file and UV positions from .obj file
  verts_uvs = aux.verts_uvs[None, ...].to(device)  # (1, V, 2)
  faces_uvs = faces.textures_idx[None, ...].to(device)  # (1, F, 3)
  image = torch.as_tensor(np.array(Image.open(f_tex).convert("RGB")) / 255.0, dtype=torch.float32).to(device)[None]
  tex = Textures(verts_uvs=verts_uvs, faces_uvs=faces_uvs, maps=image)
  
  mesh = Meshes(verts=[verts], faces=[faces.verts_idx.to(device)], textures=tex)
  
  if texture_requires_grad:
    mesh.textures._maps_padded.requires_grad = True
  
  return mesh
  
# Function executing localized FGSM attack
#unmasked_pixels: array of pixels represented as tuples of (U,V) that are subject to modification; only these pixels are altered by the iterative process (tuples are finite ordered lists of elements)
#image: tensor containing texture image data
#epsilon: hyperparameter controlling the allowed rate of texture alteration per iteration of attack
#data_grad: calculated gradient from torch.backward() pertaining to texture image data of input image
#returns: tensor containing modified texture image data
"""
def local_fgsm_attack(unmasked_pixels, image, epsilon, data_grad):
    # Collect the element-wise sign of the data gradient
    sign_data_grad = data_grad.sign()
    # Create the perturbed image by adjusting each allowed pixel of the input image
    perturbed_image = image.clone().detach().requires_grad_(True)
    #Iteratively apply gradient to each unmasked pixel
    with torch.no_grad():
      for pixel in unmasked_pixels:
        print(data_grad[0, pixel[1], pixel[0]])
        perturbed_image[0, pixel[1], pixel[0]] = perturbed_image[0, pixel[1], pixel[0]] + epsilon * data_grad[0, pixel[1], pixel[0]]  
    # Adding clipping to maintain [0,1] range, maintains original range of the data
    perturbed_image = torch.clamp(perturbed_image, 0, 1)
    # Return the perturbed image data
    return perturbed_image         
"""

def local_fgsm_attack(image, epsilon, data_grad):
    # Collect the element-wise sign of the data gradient
    sign_data_grad = data_grad.sign()
    # Create the perturbed image by adjusting each pixel of the input image
    perturbed_image = image + epsilon*sign_data_grad
    # Adding clipping to maintain [0,1] range
    perturbed_image = torch.clamp(perturbed_image, 0, 1)
    # Return the perturbed image
    return perturbed_image

##Function for data preprocessing for faster-rcnn
#
#data: rendered image of object scene
#input_size: image dimension expected by faster-rcnn
#returns: processed image data compatible with faster-rcnn
def process_data(device, data, input_size):
    data = torch.squeeze(data)
    # Rescale to [0, 1]
    data = (data - data.min()) / (data.max() - data.min())  
    mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
    std = torch.tensor([0.229, 0.224, 0.225]).to(device)
    # Normalise with tensors
    data = (data - mean[: ,None, None]) / std[: ,None, None]  
    data = torch.unsqueeze(data,0)
    # Scale to appropriate image resolution
    data = F.interpolate(data, (input_size, input_size))
    return data

##Function for truncating colors to represent integers [0, 255]
# 
#im: tensor containing texture image color data 
#returns: tensor containing truncated texture image color data to correspond to integers [0, 255]
def truncate_color_data(im):
    truncIm = ((im * 255.0) - ((im * 255) % 1)) / 255.0
    return truncIm

##Function converting a mask image into an array of unmasked pixels
#
#mask_im: mask image to be converted into array of pixels
#returns: array of tuples giving unmasked pixel coordinates
def mask_image_to_unmasked_pixel_values(mask_im):
    width, height = mask_im.size
    #Load object for direct access to pixel data
    pix = mask_im.load()
    unmasked_pixel_array = []
    for i in range(width):
      for j in range(height):
        #Add pixels of unmasked color to array of unmasked pixels as ordered coordinate pairs
        if pix[i, j] != (53, 53, 53, 0):
          unmasked_pixel_array.append((i,j))
      return unmasked_pixel_array
"""
def get_voc(root, image_set, transforms=None):
    t = [PrepareInstance()]

    if transforms is not None:
        t.append(transforms)
    t.append(tr.ToTensor())
    t.append(tr.RandomHorizontalFlip(0.5))
    transforms = tr.Compose(t)

    return dataset
"""
def Faster_RCNN(num_classes):
    anchor_generator = AnchorGenerator(sizes=(128, 256, 512), aspect_ratios=(0.5, 1.0, 2.0))
    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],
                                                    output_size=7,
                                                    sampling_ratio=2)
    true_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, 
                                                                 min_size=600, 
                                                                 max_size=1000,
                                                                #  rpn_anchor_generator=anchor_genreator,
                                                                 rpn_pre_nms_top_n_train=12000, 
                                                                 rpn_pre_nms_top_n_test=6000,
                                                                 rpn_post_nms_top_n_train=2000, 
                                                                 rpn_post_nms_top_n_test=300,
                                                                 rpn_nms_thresh=0.7,
                                                                 rpn_fg_iou_thresh=0.7, 
                                                                 rpn_bg_iou_thresh=0.3,
                                                                 rpn_batch_size_per_image=256, 
                                                                 rpn_positive_fraction=0.5,
                                                                 box_batch_size_per_image=128, 
                                                                 box_positive_fraction=0.25,
                                                                 box_score_thresh=0.1, 
                                                                 box_nms_thresh=0.3, 
                                                                 box_detections_per_img=100)

    true_model.rpn.anchor_generator = anchor_generator
    true_model.roi_heads.roi_pooler = roi_pooler
    in_features = true_model.roi_heads.box_predictor.cls_score.in_features
    true_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) 

    return true_model
# Download the pretrained Faster R-CNN model from torchvision
model = Faster_RCNN(num_classes=21)
model.to(device)
print('model construct completed. Training on %s' % device)
params = [p for p in model.parameters() if p.requires_grad]

optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

#model = models.inception_v3(pretrained=True)
pred_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
#Sets model to evaluation mode (results in layers/parts of model behaving differently such as BatchNorm layers)
pred_model.eval()

# Defining a function for get a prediction result from the model

def get_prediction(img_path, threshold):
  img = Image.open(img_path) # Load the image
  print(type(img))
  transform = tr.Compose([tr.ToTensor()]) # Defing PyTorch Transform
  #model(img.tensors)
  #img= model.transform(img)
  img = transform(img) # Apply the transform to the image
  print(type(img))
  pred = pred_model([img]) # Pass the image to the model
  print(pred)
  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())] # Get the Prediction Score
  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())] # Bounding boxes
  print(tf.shape(pred_boxes))
  print(pred_boxes)
  pred_score = list(pred[0]['scores'].detach().numpy())
  true_pred_score = [pred_score]
  global loss_tensor
  loss_tensor = torch.tensor(true_pred_score).requires_grad_()
  pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1] # Get list of index with score greater than threshold.
  pred_boxes = pred_boxes[:pred_t+1]
  pred_class = pred_class[:pred_t+1]
  return pred_boxes, pred_class

# Defining a api function for object detection

def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=1.5, text_th=3):
 
  boxes, pred_cls = get_prediction(img_path, threshold) # Get predictions
  img = cv2.imread(img_path) # Read image with cv2
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB
  for i in range(len(boxes)):
    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th) # Draw Rectangle with the coordinates
    cv2.putText(img,pred_cls[i], boxes[i][0],  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th) # Write the prediction class
    class_arr.append(pred_cls[i])
  plt.figure(figsize=(15,20)) # display the output image
  plt.imshow(img)
  plt.xticks([])
  plt.yticks([])
  plt.show()

def object_detection_api_quantized(img_path, threshold=0.5, rect_th=3, text_size=1.5, text_th=3):
 
  boxes, pred_cls = get_prediction(img_path, threshold) # Get predictions
  img = cv2.imread(img_path) # Read image with cv2
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB
  for i in range(len(boxes)):
    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th) # Draw Rectangle with the coordinates
    cv2.putText(img,pred_cls[i], boxes[i][0],  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th) # Write the prediction class
    class_quantized_arr.append(pred_cls[i])
  plt.figure(figsize=(15,20)) # display the output image
  plt.imshow(img)
  plt.xticks([])
  plt.yticks([])
  plt.show()

# Define the class names given by PyTorch's official Docs

COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',
    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',
    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']

########################## ESTABLISH PATHS TO DATA ##########################
#Try Faster-RCNN without loop
#List variables

BASE_PATH = '/content/drive/MyDrive/'
RESULTS_FOLDER = 'results'
MODEL3D_NAME = 'couch'
MODEL3D_FOLDER = '3d_models'
MASK_NAME = 'couch'
TEXTURE_NAME = 'couch.png'
true_num_target = 0
MODEL3D_TARGET = 'couch'
SAVED_INTERMEDIATE_TEXTURES_FOLDER = 'saved_intermediate_textures'
EPSILON = 0.05
NUM_ITERATIONS = 1
input_size = 299
epsilon = EPSILON

MODEL_PATH = os.path.join(BASE_PATH, MODEL3D_FOLDER, MODEL3D_NAME+'.obj')

TEXTURE_PATH = os.path.join(BASE_PATH, MODEL3D_FOLDER, TEXTURE_NAME)

RESULTS_PATH = os.path.join(BASE_PATH, RESULTS_FOLDER)
if not os.path.exists(RESULTS_PATH):
    os.makedirs(RESULTS_PATH)

RESULTS_MODEL3D_PATH = os.path.join(RESULTS_PATH, MODEL3D_NAME)
if not os.path.exists(RESULTS_MODEL3D_PATH):
    os.makedirs(RESULTS_MODEL3D_PATH)
    
SAVED_INTERMEDIATE_TEXTURES_PATH = os.path.join(RESULTS_MODEL3D_PATH, SAVED_INTERMEDIATE_TEXTURES_FOLDER)
if not os.path.exists(SAVED_INTERMEDIATE_TEXTURES_PATH):
    os.makedirs(SAVED_INTERMEDIATE_TEXTURES_PATH)
    
MASK_PATH = os.path.join(BASE_PATH, MODEL3D_FOLDER, MASK_NAME+'.png')

# Test for saving images to Google Drive (Part 1)
IMAGES_FOLDER = 'images'
IMAGES_PATH = os.path.join(RESULTS_MODEL3D_PATH, IMAGES_FOLDER)
if not os.path.exists(IMAGES_PATH):
    os.makedirs(IMAGES_PATH)

########################## SCENE PREPARATION ##########################

#Establish multiview setting for rendering from many angles and distances
#Set ambient color to white
ambient_color = 1.0
#Establish distance of camera from spherial coordinates of distance, elevation, and azimuth
camera_distances = [3]
camera_elevations = [20]
camera_azimuths = [0] 

#Set high rendering resolution for high detail
image_size = 1024   
faces_per_pixel = 10

#Find size of each rendering batch (to be used as denominator in attack success ratio)
num_renders_per_iteration = len(camera_distances) * len(camera_elevations) * len(camera_azimuths)
print(num_renders_per_iteration)
"""
#Load Image and Pixel Mask
#Load and process mask to obtain array of pixels to be adversarially perturbed
mask_im = Image.open(MASK_PATH, 'r')
#Returns tuples representing (x,y) values for unmasked pixels
unmasked_pixel_array = mask_image_to_unmasked_pixel_values(mask_im) 

#Set up Renderer
raster_settings = RasterizationSettings(image_size=image_size,
                                        blur_radius=0.0,
                                        faces_per_pixel=faces_per_pixel,
                                        bin_size=None,
                                        max_faces_per_bin=None
)

renderer = MeshRenderer(
    rasterizer=MeshRasterizer(
        cameras=None,
        raster_settings=raster_settings,
    ),
    shader=SoftPhongShader(
        device=device,
        cameras=None,
        lights=None,
    )
)
"""
sigma = 1e-4
raster_settings = RasterizationSettings(
    image_size=128, 
    blur_radius=np.log(1. / 1e-4 - 1.)*sigma, 
    faces_per_pixel=50, 
)

# Silhouette renderer 
renderer = MeshRenderer(
    rasterizer=MeshRasterizer(
        cameras=None, 
        raster_settings=raster_settings_silhouette
    ),
    shader=SoftSilhouetteShader()
)

#Load 3D objects and texture
print(TEXTURE_PATH)
mesh = load_obj_and_texture_img_as_mesh(MODEL_PATH, TEXTURE_PATH, device=device)
texture_tensor = mesh.textures._maps_padded
texture_tensor.requires_grad = True
#Save orginal texture data tensor for later comparsion
texture_tensor_before = texture_tensor.data.clone()

#EXECUTE LOCAL ATTACK
#Set the target for use in cross-entropy loss function 
target = Variable(torch.LongTensor([true_num_target]), requires_grad=False)
target = target.to(device)

#Declare variable to track attack success rate
success_ratio = 0
#Declate variable to track attack success rate after truncating texture color data to save-friendly format
quantized_success_ratio = 0
accum_texture_grad = None
#Boolean variables to test perturbed texture's attack robustness after saving texture as image file
try_quantize = False
quantized = False

print(tf.shape(target))

"""
#Calculate the loss
for epoch in range(3):
  loss_epoch = {}
  loss_name = ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']
  for ii, (images, targets) in tqdm(enumerate(dataloader)):
    model.train()
    model.zero_grad()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v, in t.items()} for t in targets]
    loss_dict = model(images, targets)
    losses = sum(loss for loss in loss_dict.values())
    losses.backward()
    optimizer.step()
"""

#Localized attack loop
k = 1
while k <= NUM_ITERATIONS and not quantized:
  k=k+1
  #Test robustness of truncated save-friendly data if torch.tensor float data attack success rate is 100%
  if success_ratio == 1.0:
    try_quantize = True
  #Call FGSM Attack and update texture in mesh
  if k > 1 and accum_texture_grad is not None:
    #Apply texture gradient to pixels in texture image not covered by mask
    #perturbed_texture = local_fgsm_attack(unmasked_pixel_array, texture_tensor, epsilon, accum_texture_grad)
    perturbed_texture = local_fgsm_attack(texture_tensor, epsilon, accum_texture_grad)
    texture_tensor.data = perturbed_texture
    #Save the perturbed texture to disk in image format
    im = Image.fromarray((perturbed_texture[0].detach().cpu().numpy() * 255.0).astype(np.uint8))
    im.save(os.path.join(SAVED_INTERMEDIATE_TEXTURES_PATH, "couch_texture_" + str(datetime.now()) + ".png")) # use for step 1
  
  #Clear accumulated texture gradient between iterations
  accum_texture_grad = None

  #Numerators for success ratio for attack iteration
  count = 0
  quantized_count = 0
  #Establish scene ambient light effect
  materials = Materials(device=device, ambient_color=[[ambient_color] * 3])

  for dist in camera_distances:
    for elev in camera_elevations:
      for azim in camera_azimuths:
        #Zero model gradients (set to 0)
        model.zero_grad()
        renderer.zero_grad()
        if texture_tensor.grad is not None:
          texture_tensor.grad.data.zero_()
        class_arr = []
        class_quantized_arr = []

        #Set camera and lighting position based on spherical coordinates (distance, elevation, and azimuth)
        R, T = look_at_view_transform(dist, elev, azim)
        cameras = OpenGLPerspectiveCameras(device=device, R=R, T=T)
        location = cameras.get_camera_center().detach().cpu().numpy()
        lights = PointLights(device=device, location = location, ambient_color=((1, 1, 1),), diffuse_color=((0.2, 0.2, 0.2),))
        
        #Run differentiable renderer and process output for training targets
        # Render silhouette images.  The 3rd channel of the rendering output is 
        # the alpha/silhouette channel
        images = renderer(mesh, cameras=cameras, lights=lights)
        target_silhouette = [silhouette_images[i, ..., 3] for i in range(num_renders_per_iteration)]
        """
        images = renderer(mesh, cameras=cameras, materials=materials,  lights = lights)
        images = images[..., :3]
        """
        images = images.permute(0, 3, 1, 2)
        torch.narrow(images, 1, 0, 2)
        print(images)
        print(tf.shape(images.detach().numpy()))
        print(target_silhouette)
        #Test data's robustness in save-friendly format if attack rate already at 100% with raw data
        if try_quantize:
          #Generate texture image data in save fuction format
          quantized_image = truncate_color_data(images)
          #Prepare data for faster-rcnn forward pass
          #quantized_processed_temp_images = process_data(device, images, input_size)
          img_quantized_test_path = os.path.join(IMAGES_PATH, "couch_quantized_" + str(k) + ".png")
          utils.save_image(quantized_image, img_quantized_test_path)
          object_detection_api(img_quantized_test_path,threshold=0.8)

        #Prepare data for Inception v3 forward pass
        #processed_temp_images = process_data(device, images, input_size)
        #saves image to disk
        """
        model.train()
        model.zero_grad()
        loss_dict = model(images, target)
        losses = sum(loss for loss in loss_dict.values())
        losses.backward()
        optimizer.step()
        print('NEWEST GRAD')
        print(texture_tensor.grad)
        """
        img_test_path = os.path.join(IMAGES_PATH, "couch_" + str(k) + ".png")
        utils.save_image(images, img_test_path)
        object_detection_api(img_test_path,threshold=0.8)

        if class_arr[0] != MODEL3D_TARGET or len(class_arr) != 1:
          print('dist: {}, elev: {}, azim: {}, false negative - prediction: {}'.format(dist, elev, azim, class_arr[0], len(class_arr)))
          #Increment attack success counter
          count = count + 1
          #Calculate loss again if perturbation robustness not maintained in save-friendly format
          if try_quantize and class_quantized_arr[0] != class_arr[0]:
            # Calculate the loss
            loss_fun = torch.nn.CrossEntropyLoss()
            loss = loss_fun(loss_tensor, target)
            
            #Calculate gradients of model in backward pass
            loss.backward()

            #Collect datagrad and accumulate
            if accum_texture_grad is None:
              accum_texture_grad = texture_tensor.grad.data.detach().clone()
            else:
              accum_texture_grad += texture_tensor.grad.data.detach().clone()
          elif try_quantize and class_quantized_arr[0] == class_arr[0]:
            #Increment attack success counter
            quantized_count = quantized_count + 1
        else:
          print('dist: {}, elev: {}, azim: {}, true - prediction: {}'.format(dist, elev, azim, class_arr[0], len(class_arr)))
          loss_fun = torch.nn.CrossEntropyLoss()
          loss = loss_fun(loss_tensor, target)

          #Calculate gradients of model in backward pass
          loss.backward()
          
          #Collect datagrad and accumulate
          if accum_texture_grad is None:
            print(loss)
            print(texture_tensor.is_leaf)
            print('DATA')
            print(texture_tensor.data)
            print('GRAD')
            print(texture_tensor.grad)
            print('TRUE GRAD')
            #true_grad = tf.zeros([1, 1024, 1024, 3])

            #texture_tensor.grad = true_grad
            accum_texture_grad = texture_tensor.grad.data.detach().clone()
            #accum_texture_grad = true_grad
          else:
            accum_texture_grad += texture_tensor.grad.data.detach().clone()

  #Calculate attack success rate
  success_ratio = count/num_renders_per_iteration
  print('{}-iteration - attack rate: {}'.format(k, success_ratio))
  #Check robustness of attack in save-friendly foramt if raw texture image tensor data produced 
  #100% attack success rate in previous iteration
  if try_quantize:
    #Calculate attack success rate in save-friendly format
    quantized_success_ratio = quantized_count/num_renders_per_iteration
    print('{}-iteration - attack rate: {}'.format(k, quantized_success_ratio))
    #Enable attack loop to end if robustness is maintained
    if quantized_success_ratio == 1.0:
      quantized = True

"""
from google.colab import drive
root = '/content/drive/'
drive.mount(root)

# Making Directory

import os 
from os.path import join

mot = "My Drive/Colab Notebooks/MOT/"   # a custom path. you can change if you want to
MOT_PATH = join(root,mot)
!mkdir "{MOT_PATH}"

# Download MOT17Det Dataset

!wget -P "{MOT_PATH}" https://motchallenge.net/data/MOT17Det.zip
!cd "{MOT_PATH}";unzip MOT17Det.zip

# Remove unwanted data for drive volume issue (optional)

!cd "{MOT_PATH}";rm -rf test
!cd "{MOT_PATH}";rm -rf train/MOT17-02;rm -rf train/MOT17-04;rm -rf train/MOT17-05
!cd "{MOT_PATH}";rm -rf train/MOT17-10;rm -rf train/MOT17-11;rm -rf train/MOT17-13

"""
# Detection information on all the images is well-refined as a json file, which is available at our course git repo

!cd "{MOT_PATH}";git clone https://github.com/mlvlab/COSE474.git

jsonpath = join(MOT_PATH,'COSE474/3_MOT_detinfo.json')

with open(jsonpath) as data_file:    
   data = json.load(data_file)
odata = collections.OrderedDict(sorted(data.items()))

# Let's check out downloaded json file

pprint(odata)

img_path = motdata    # img root path

# Making new directory for saving results
save_path = join(MOT_PATH,'save/')
!mkdir "{save_path}"

mot_tracker = Sort()      # Tracker using SORT Algorithm

for key in odata.keys():   
    arrlist = []
    det_img = cv2.imread(os.path.join(img_path, key))
    overlay = det_img.copy()
    det_result = data[key] 
    
    for info in det_result:
        bbox = info['bbox']
        labels = info['labels']
        scores = info['scores']
        templist = bbox+[scores]
        
        if labels == 1: # label 1 is a person in MS COCO Dataset
            arrlist.append(templist)
            
    track_bbs_ids = mot_tracker.update(np.array(arrlist))
    
    mot_imgid = key.replace('.jpg','')
    newname = save_path + mot_imgid + '_mot.jpg'
    print(mot_imgid)
    
    for j in range(track_bbs_ids.shape[0]):  
        ele = track_bbs_ids[j, :]
        x = int(ele[0])
        y = int(ele[1])
        x2 = int(ele[2])
        y2 = int(ele[3])
        track_label = str(int(ele[4])) 
        cv2.rectangle(det_img, (x, y), (x2, y2), (0, 255, 255), 4)
        cv2.putText(det_img, '#'+track_label, (x+5, y-10), 0,0.6,(0,255,255),thickness=2)
        
    cv2.imwrite(newname,det_img)